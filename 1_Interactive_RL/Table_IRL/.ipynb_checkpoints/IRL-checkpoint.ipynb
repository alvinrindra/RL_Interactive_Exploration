{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Declaration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classes.Variables import *\n",
    "reward = 1\n",
    "punishment = -1\n",
    "feedbackstrategy_random = 0\n",
    "feedbackstrategy_early = 1\n",
    "feedbackstrategy_importance = 2\n",
    "feedbackstrategy_correction = 3\n",
    "\n",
    "# optimal mistake correcting importance threshold\n",
    "importancethreshold =  0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classes.Scenario import Scenario\n",
    "class Scenario(object):\n",
    "    \n",
    "    actualState = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        #Neural network weights Deep MLP 2x46x8x1\n",
    "        self.iw = np.mat(\"6.9238164774\t7.2590406121\t;-11.0642990177\t-4.5679442843\t;-13.9739059084\t4.1848153426\t;-8.8791470291\t5.3048056315\t;10.4742453858\t3.9665173236\t;-21.0854579507\t10.2017875554\t;7.3405884608\t-9.5310376703\t;-10.1011106686\t-2.3163227587\t;-23.1235920421\t10.0241597141\t;-10.7564741817\t0.8848330739\t;-9.0475999044\t-6.2853315008\t;-11.0782890879\t7.4075081067\t;1.9128471504\t11.3423562891\t;-10.8463987888\t-4.5174329044\t;1.6413761867\t-10.301994201\t;3.8604046748\t9.3304565466\t;12.8243530351\t-6.6217328292\t;10.2104881333\t-9.3820715769\t;9.1688628483\t9.112648532\t;-8.9045422391\t8.9205589871\t;-14.8552357987\t-7.3945441056\t;9.3089286607\t8.235215213\t;10.0949068503\t-7.8050220757\t;-0.1540096846\t10.0196692174\t;-10.5067179699\t10.5139376702\t;-15.3361990344\t0.3652767078\t;-9.4654923174\t-6.8637592767\t;5.8394713682\t-10.1876676661\t;6.6100796729\t-4.8965417508\t;12.1236125103\t4.7154855681\t;-22.162842182\t-0.5083451458\t;-24.3371531842\t0.8390278135\t;5.1775858551\t11.4212773966\t;7.3658536122\t-10.3613012331\t;9.1030093922\t-6.2895619736\t;11.5125184372\t-5.5666006298\t;10.1642196393\t-5.0661609667\t;11.1650805102\t-6.3021726979\t;9.8845967996\t-3.0549253685\t;-11.5777546154\t-17.4392035535\t;6.525157989\t-5.2378575584\t;-14.860326498\t-4.7764609252\t;8.9422775634\t7.4877018945\t;-5.7577559467\t6.1732954024\t;8.2603773319\t4.1849873275\t;10.4921560944\t4.3022698661\")\n",
    "        self.lw1 = np.mat(\"0.8740896112\t-0.654229184\t1.3340632886\t-0.2082553465\t-1.0793655553\t1.7302181759\t-0.4106983628\t1.8727862415\t1.0339512278\t-0.4949443637\t2.9072466214\t-0.8345920904\t-1.4699036101\t0.6073815977\t1.9921252282\t-0.2853743291\t3.1752436831\t-1.9182921124\t1.5133789968\t-0.8998314074\t-1.1439569142\t2.3878036978\t-0.0704629993\t-1.2071823003\t0.3553881578\t1.2909436034\t-0.9289864364\t0.7168112231\t3.2693414501\t0.7025777143\t4.2068370617\t2.4502670944\t-0.6149442187\t1.9977374259\t-0.1846960349\t-1.3818868027\t-0.4401159527\t-2.3031517689\t0.936143867\t-2.1874766417\t-0.9060227023\t-0.5742953297\t0.6773463487\t-0.285823865\t0.6001369982\t1.0859852787\t;-1.4508413855\t0.7662097708\t1.5909325021\t1.1275569528\t-0.5039532492\t0.6880134582\t2.570027639\t1.1283563225\t-0.4555130288\t0.6341915446\t6.41756643\t-0.6580561805\t1.998874904\t-2.7348208075\t2.0270436925\t0.4986154939\t3.9706189092\t-3.8928533514\t0.9391543779\t-4.979007404\t2.1844874554\t-0.2194698783\t0.9702272877\t1.7746625543\t1.3206569257\t-5.4948354654\t-1.0850663169\t1.0110046424\t-0.9568377059\t2.1363904676\t11.194993415\t-8.2620628601\t1.8566317027\t3.6524152033\t0.6120978692\t-0.2420542723\t1.0215296101\t-0.1244260738\t-2.9653582457\t3.4860974974\t-0.0243108977\t0.6374052757\t5.0698022352\t0.4504356435\t-1.9268361582\t-1.6276081257\t;-2.1523685704\t1.4796831986\t1.5367967533\t0.1425404042\t-0.069810081\t-1.1333004438\t0.1591686163\t3.4120000318\t-3.5830310024\t-0.8610457379\t0.4148668826\t3.5727297589\t-1.3930906441\t-0.4080366918\t1.2355409057\t3.7376137279\t1.1791667016\t2.5517154966\t1.0807332407\t-1.969291399\t7.5111120942\t0.9609890826\t-2.3509709725\t0.5472426332\t2.4380431515\t-1.3916147246\t4.7096051392\t1.9205906783\t1.454260192\t0.4439443667\t-4.9079107015\t-0.8720898707\t-0.9644199479\t2.1543670601\t-1.0882006221\t-2.342581976\t0.3237030828\t-1.5926352555\t1.2242215831\t-0.806442926\t0.9099422407\t0.1642906345\t0.7642144815\t-2.4231519381\t0.533548319\t0.2287500606\t;-4.2883289215\t-3.4802584935\t3.5253767255\t0.9137391772\t5.2742815987\t3.1113868777\t4.1244071739\t3.6413999024\t0.0102081774\t-5.8155531418\t0.2842338333\t-7.8925593817\t-1.5578461529\t0.3897249044\t1.9529054927\t-4.0840020949\t-4.1260356287\t-6.2130366108\t-5.9702295483\t-2.4983842155\t4.03918155\t3.1277698078\t4.0269712857\t1.9654089529\t2.5658213457\t-3.6300789057\t0.4426562552\t-0.0931050323\t-0.7792276488\t2.559985865\t11.2575729483\t-8.502249391\t2.1895978941\t3.0030469476\t-0.7651985153\t2.3453985913\t-2.1245627141\t1.967105804\t-0.8162418349\t2.2804244354\t0.130418045\t1.3162590864\t5.3429624904\t0.489159331\t-2.8692741738\t-1.298877034\t;-3.1447096469\t-0.3470774764\t1.1944565465\t2.6932236618\t1.0338415515\t0.606268051\t1.4527866642\t0.057169096\t-4.4683848861\t0.246697107\t1.8773975795\t3.3099561233\t5.8279680344\t-3.2418087958\t-1.4739736693\t2.0628233424\t-2.7259687205\t3.4560602909\t-1.9112898146\t-2.7959767995\t-1.8204832353\t0.128095842\t0.5812772774\t2.6456042328\t1.7427798668\t0.8534991002\t-1.8487438821\t0.2845915004\t5.0775257341\t0.7790485733\t-9.7901293218\t9.408153783\t-5.3171316637\t0.3109206524\t2.5782935162\t-5.8290104402\t-0.9813381617\t-2.3910869173\t1.3790237968\t2.3087221596\t-1.0490694964\t1.3678040613\t-0.94445633\t2.1950442548\t-0.8514840362\t3.0744439647\t;1.982324184\t1.5791034562\t19.0967045031\t2.1043923214\t-1.2075875002\t-8.7285840847\t2.3175970206\t0.6435285036\t-11.4047071859\t-2.0343467458\t2.3140016268\t1.5759252344\t4.4654771294\t-6.7162288086\t2.1169820046\t0.2541835204\t-5.4472324273\t3.1524285267\t-0.3795618343\t3.7371868178\t-2.6581498161\t-1.1307125688\t0.1930718465\t0.473512012\t-0.1847819117\t0.3361656227\t-0.6769841183\t0.600282991\t2.598286397\t0.2572135284\t-6.4505065754\t5.9814018053\t7.983653868\t-3.3335640336\t-1.5351661372\t-0.2453214528\t-3.8619088194\t1.0010389513\t-0.9989953825\t11.9865712681\t3.533751156\t2.125117457\t-1.9856925556\t-0.0591690032\t1.5519961016\t2.1623480688\t;-5.5804248885\t-2.1727515999\t-5.6419404629\t-4.9103170792\t0.5500381267\t7.3987342498\t-1.5861958376\t0.7345746026\t2.9966145011\t-6.6543996458\t5.4277100744\t-3.6639731806\t1.0999605431\t6.1400494727\t-2.453204562\t-0.8167191211\t-1.6039924251\t-2.1811250062\t0.0195169893\t-1.6751087256\t2.7097368445\t5.8800770114\t2.423838724\t0.4229037805\t0.086622211\t-0.2939795537\t4.4049725239\t-4.4630311306\t0.1609757767\t-0.6268777046\t-3.4013403052\t-2.1574949543\t-2.5610452068\t0.8769335348\t3.235560725\t1.1038642967\t1.1407272193\t-0.061642651\t1.2697240335\t1.6182946172\t-2.9025063853\t-2.2547792517\t-0.5097734722\t0.0279344144\t0.1348776347\t-2.3616462881\t;-0.0235965602\t-0.5228784042\t1.2687912414\t1.3885763149\t0.6596401817\t-0.0114573794\t-2.4167474376\t-2.4514078311\t-1.6536539367\t-0.4943612209\t1.348702281\t-1.2076123256\t4.6239039061\t-1.3182315508\t-0.6546611623\t-3.5985521735\t2.3839140911\t0.5264152968\t5.7717705529\t0.3154204971\t2.8546835371\t4.6235317599\t-2.92414094\t-0.4417555648\t-3.6684787568\t-1.8314964066\t1.4188570485\t-0.6388752856\t3.7243380165\t-3.3717187307\t0.8230254373\t-0.2889777815\t3.5072558727\t-2.9654598577\t1.3566857282\t-0.9354168219\t0.5857940785\t-1.6018361153\t2.7538632232\t-1.1539296053\t-3.7977634462\t0.5954279002\t-0.2353850974\t0.0320212764\t0.1151419878\t-0.3431359481\")\t\n",
    "        self.lw2 = np.mat(\"-3.2821818943\t7.3053343313\t-4.1283213449\t-7.3449881479\t-4.1294170907\t7.5369876707\t7.5369868066\t-3.4471658817\")\n",
    "        self.b1 = np.mat(\"-9.6094512061\t;9.1090731419\t;11.5603090511\t;9.7970653538\t;-8.9510414758\t;17.2594473983\t;-8.5515552973\t;7.7204909178\t;14.6625356379\t;6.4604773088\t;5.0093544574\t;6.9958161631\t;5.9002704597\t;6.1107770806\t;-4.0518466071\t;-4.1805174969\t;-7.4569659451\t;-4.1938428979\t;-3.7406949043\t;1.6361180434\t;0.9627144182\t;-1.2997822743\t;-0.7209345562\t;-0.8694880172\t;0.6717353627\t;0.6798894339\t;-0.5001321717\t;2.1552472224\t;2.8949802519\t;4.7499923609\t;-7.2905731067\t;-8.014666727\t;6.355387244\t;7.1851143483\t;7.4583997827\t;4.7198083671\t;5.5584595945\t;7.1523636529\t;8.0752154138\t;-14.4516791729\t;6.590585438\t;-12.0124241864\t;10.7418138732\t;-9.2735632828\t;9.2005977087\t;10.6381185646\")\n",
    "        self.b2 = np.mat(\"2.3613797973\t;2.3047108024\t;1.260206902\t;-1.8886456859\t;-0.6464332401\t;0.2879900144\t;-0.7460862889\t;1.4745923483\")\n",
    "        self.b3 = np.mat(\"-1.1264269344\")\n",
    "        \n",
    "        self.actions = 8\n",
    "        self.states = 46\n",
    "    #end of __init__ method\n",
    " \n",
    "    def getNumberOfStates(self):\n",
    "        return self.states\n",
    "    #end of getOfStates method\n",
    "\n",
    "    def getNumberOfActions(self):\n",
    "        return self.actions\n",
    "    #end of getOfStates method\n",
    "\n",
    "    def getTransition(self, state, action):\n",
    "        state = state+1\n",
    "        action = action+1\n",
    "        stateNormalized =  2*(state-1)/45.0 - 1;\n",
    "        actionNormalized =  2*(action-1)/7.0 - 1;\n",
    "        inputData = np.matrix([[stateNormalized], [actionNormalized]])\n",
    "        hiddenValues1 = np.tanh(self.iw * inputData + self.b1);\n",
    "        hiddenValues2 = np.tanh(self.lw1 * hiddenValues1 + self.b2);\n",
    "        outputData = self.lw2 * hiddenValues2 + self.b3;\n",
    "        outputDesnormalized = int(np.round((outputData + 1)*23 - 1))\n",
    "        return outputDesnormalized\n",
    "    #end of getTransition method\n",
    "\n",
    "    def resetScenario(self):\n",
    "        self.actualState = 0\n",
    "    #end of resetScenario method\n",
    "\n",
    "    def getState(self):\n",
    "        return self.actualState\n",
    "    #end of getState method\n",
    "    \n",
    "    def executeAction(self, action):\n",
    "        state = self.getState()\n",
    "        self.actualState = self.getTransition(state, action)\n",
    "    #end of executeAction method\n",
    "    \n",
    "    def getReward(self):\n",
    "        if self.actualState == -1:\n",
    "            return -1\n",
    "        elif self.actualState == 45:\n",
    "            return 1\n",
    "        else:\n",
    "            return -0.01\n",
    "#end of class Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classes.Agent import Agent\n",
    "class Agent(object):\n",
    "    alpha = 0.3  # 0.1 #0.7\n",
    "    gamma = 0.9  # 0.4\n",
    "    epsilon = 0.1  # 0.25\n",
    "\n",
    "    def __init__(self, scenario):\n",
    "        self.scenario = scenario\n",
    "        self.numberOfStates = self.scenario.getNumberOfStates()\n",
    "        self.numberOfActions = self.scenario.getNumberOfActions()\n",
    "        self.Q = np.random.uniform(0.0, 0.01, (self.numberOfStates, self.numberOfActions))\n",
    "        self.feedbackAmountTotal = 0\n",
    "        self.feedbackAmountEpisode = 0\n",
    "\n",
    "    # end of __init__ method\n",
    "\n",
    "    def selectAction(self, state):\n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            action = np.random.randint(self.numberOfActions)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state, :])\n",
    "        # endIf\n",
    "        return action\n",
    "\n",
    "    # end of selectAction method\n",
    "\n",
    "    def actionByFeedback(self, state, teacherAgent, feedbackStrategy, feedbackParameter):\n",
    "        if (feedbackStrategy == Variables.feedbackstrategy_random):\n",
    "            if (np.random.rand() < feedbackParameter):\n",
    "                # get advice\n",
    "                action = np.argmax(teacherAgent.Q[state, :])\n",
    "                self.feedbackAmountEpisode += 1\n",
    "            else:\n",
    "                action = self.selectAction(state)\n",
    "        elif (feedbackStrategy == Variables.feedbackstrategy_early):\n",
    "            if (self.feedbackAmountEpisode < feedbackParameter):\n",
    "                # get advice\n",
    "                action = np.argmax(teacherAgent.Q[state, :])\n",
    "                self.feedbackAmountEpisode += 1\n",
    "            else:\n",
    "                action = self.selectAction(state)\n",
    "\n",
    "        elif (feedbackStrategy == Variables.feedbackstrategy_importance):\n",
    "            if (self.feedbackAmountEpisode < feedbackParameter and teacherAgent.isImportant(state)):\n",
    "                # get advice\n",
    "                action = np.argmax(teacherAgent.Q[state, :])\n",
    "                self.feedbackAmountEpisode += 1\n",
    "            else:\n",
    "                action = self.selectAction(state)\n",
    "\n",
    "        elif (feedbackStrategy == Variables.feedbackstrategy_correction):\n",
    "            teacheraction = np.argmax(teacherAgent.Q[state, :])\n",
    "            selfaction = self.selectAction(state)\n",
    "            if (self.feedbackAmountEpisode < feedbackParameter and teacherAgent.isImportant(state) and teacheraction != selfaction):\n",
    "                # get advice\n",
    "\n",
    "                action = teacheraction\n",
    "\n",
    "                self.feedbackAmountEpisode += 1\n",
    "            else:\n",
    "                action = selfaction\n",
    "\n",
    "        ####\n",
    "        # endIf\n",
    "        return action\n",
    "\n",
    "    # end of actionByFeedback\n",
    "\n",
    "    def isImportant(self, state):\n",
    "\n",
    "        # importance by range\n",
    "        # importance = max(self.Q[state, :]) - min(self.Q[state, :])\n",
    "\n",
    "        # importance by total deviation\n",
    "        importance = np.mean(np.absolute(self.Q[state, :] - np.mean(self.Q[state,:])))\n",
    "        # print(importance)\n",
    "\n",
    "        if importance > Variables.importancethreshold:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # end of isImportant\n",
    "\n",
    "    def train(self, episodes, teacherAgent=None, feedbackStrategy=0, feedbackParameter=0):\n",
    "        contCatastrophic = 0\n",
    "        contFinalReached = 0\n",
    "        steps = np.zeros(episodes)\n",
    "        rewards = np.zeros(episodes)\n",
    "        failures = np.zeros(episodes)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            contSteps = 0\n",
    "            accReward = 0\n",
    "            success = 0\n",
    "            self.feedbackAmountEpisode = 0\n",
    "            self.scenario.resetScenario()\n",
    "            state = self.scenario.getState()\n",
    "            action = self.actionByFeedback(state, teacherAgent, feedbackStrategy, feedbackParameter)\n",
    "\n",
    "            # expisode\n",
    "            while True:\n",
    "                # perform action\n",
    "                self.scenario.executeAction(action)\n",
    "                contSteps += 1\n",
    "\n",
    "                # get reward\n",
    "                reward = self.scenario.getReward()\n",
    "                accReward += reward\n",
    "                # catastrophic state\n",
    "\n",
    "                stateNew = self.scenario.getState()\n",
    "\n",
    "                if reward == Variables.punishment:\n",
    "                    contCatastrophic += 1\n",
    "                    self.Q[state, action] = -0.1\n",
    "                    break\n",
    "\n",
    "                actionNew = self.actionByFeedback(stateNew, teacherAgent, feedbackStrategy, feedbackParameter)\n",
    "\n",
    "                # updating Q-values\n",
    "                self.Q[state, action] += self.alpha * (reward + self.gamma *\n",
    "                                                       self.Q[stateNew, actionNew] -\n",
    "                                                       self.Q[state, action])\n",
    "\n",
    "                if reward == Variables.reward:\n",
    "                    contFinalReached += 1\n",
    "                    success = 1\n",
    "                    break\n",
    "\n",
    "                state = stateNew\n",
    "                action = actionNew\n",
    "            # end of while\n",
    "\n",
    "            rewards[i] = accReward\n",
    "            if success == 1:\n",
    "                steps[i] = contSteps\n",
    "                failures[i] = 0\n",
    "            else:\n",
    "                steps[i] = 0\n",
    "                failures[i] = 1\n",
    "\n",
    "\n",
    "            self.feedbackAmountTotal += self.feedbackAmountEpisode\n",
    "\n",
    "        # end of for\n",
    "        print(\"Total feedback amount in all episodes:\" + str(self.feedbackAmountTotal))\n",
    "        print(\"Total amount of failures\" + str(contCatastrophic))\n",
    "        print(\"Total amount of successes\" + str(contFinalReached))\n",
    "\n",
    "        return steps, rewards, failures\n",
    "        # end of train method\n",
    "\n",
    "# end of class Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classes.DataFiles import DataFiles\n",
    "class DataFiles(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "    #end of __init__ method\n",
    "\n",
    "    def createFile(self, filename):\n",
    "        myFile=open(filename,'w')\n",
    "        myFile.close()\n",
    "    \n",
    "    def addToFile(self, filename, var):\n",
    "        myFile=open(filename,'a')\n",
    "        for i in range(len(var)-1):\n",
    "            myFile.write(str(int(var[i]))+',')\n",
    "    \n",
    "        myFile.write(str(int(var[len(var)-1]))+'\\n')\n",
    "        myFile.close()\n",
    "\n",
    "    def addFloatToFile(self, filename, var):\n",
    "        myFile=open(filename,'a')\n",
    "        for i in range(len(var)-1):\n",
    "            myFile.write(str(var[i])+',')\n",
    "    \n",
    "        myFile.write(str(var[len(var)-1])+'\\n')\n",
    "        myFile.close()\n",
    "    \n",
    "    def readFile(self, filename):\n",
    "        myFile=open(filename,'r')\n",
    "        line=myFile.readline()\n",
    "        dataFile = []\n",
    "        while line != \"\":\n",
    "            data = line.split(',')\n",
    "            dataInt = []\n",
    "            for i in range(len(data)):\n",
    "                dataInt.append(int(data[i]))\n",
    "            \n",
    "            dataFile.append(dataInt)\n",
    "            line = myFile.readline()\n",
    "        myFile.close()\n",
    "        return dataFile\n",
    "\n",
    "    def readFloatFile(self, filename):\n",
    "        myFile=open(filename,'r')\n",
    "        line=myFile.readline()\n",
    "        dataFile = []\n",
    "        while line != \"\":\n",
    "            data = line.split(',')\n",
    "            dataInt = []\n",
    "            for i in range(len(data)):\n",
    "                dataInt.append(float(data[i]))\n",
    "            \n",
    "            dataFile.append(dataInt)\n",
    "            line = myFile.readline()\n",
    "        myFile.close()\n",
    "        return dataFile\n",
    "\n",
    "#end of class DataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
